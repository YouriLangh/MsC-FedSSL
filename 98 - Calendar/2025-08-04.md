We have model collapse yet again when lambda_u = 1 and 1% of data, how are we performin worse than when lambda_u = 0 (meaning supervised clients with 0 data)


**TODO**
Validation with optuna & k-fold (this means that data splitting has to be done before already kinda, since trainloader cant just take from the folder) This means that train_data in fedssl or supervised baseline has to be from a function which i alter.
~~**Test whether the data splitting is the issue.**~~
~~**Add most recent one when using non-ordered dicts rather than the highest confidence.**~~
~~**Adapative threshold & pretraining bug**~~
~~Think about tests & run some to have results ~~
Recap & start writing
Test sample project on hpc from Simon 
- Include more & more libraries n then switch to my code.
~~**Add U_ratio again for batchloading more unlabeled than labeled**~~
*Add ability to split the data by nr labels/ class perhaps and then shuffle it later? Ask simon abt this (rather than just making it smaller, make it smaller in a certain way) --> No. This is more realistic.*
Model collapse again...
Add ability to store different data for all teh tests:
Use 1 file per run id and then a helper.
 ```python
all_results = []
for alpha in [0.1, 0.5, 1.0]:
    for labeled_fraction in [0.01, 0.05, 0.1]:
        for lambda_u in [0.0, 0.1, 0.7, 1.0]:
            result = run_experiment(alpha, labeled_fraction, lambda_u)
            all_results.append(result)

```
==> Alter the visualization code & result saving code.
Alter results saving code to perhaps have some identifying fields for them as can be found ion [[Store tests]]. Like 'lambda_u : 0.1' etc
Add code to run all the tests
Add normalizations for each dataset:
Cifar10: T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])


*Questions:*
*Shouldn't lambda be on aschedule to if threshold on a schedule? Perhaps weight it by amount of data we have and maybe alter stm to also follow this approach where it gets bigger over time?*
*Could use confidence weighted loss:*
And add is as ablation perhaps lter with an iftest
*unsupervised_loss = (raw_loss * confidences * mask).sum() / mask.sum()*
*Cycle approach as i saw [someone](https://github.com/kekmodel/FixMatch-pytorch/blob/master/train.py#L454) Or current approach is better*


Mnist:

| Lambda_u | Labeled % | Test Acc | F1 Macro |
| -------- | --------- | -------- | -------- |
| 0.7      | 0.01      | **0.30** | 0.22     |
| 0.0      | 0.01      | **0.51** | 0.48     |
| 0.1      | 0.01      | 0.45     | 0.45     |
| 1        | 0.01      | 0.322    | 0.214    |

Cifar10:

| Lambda_u | Labeled % | Test Acc | F1 Macro |
| -------- | --------- | -------- | -------- |
| 0.7      | 0.01      | 0.20     | 0.095    |
| 0.0      | 0.01      | 0.20     | 0.095    |
| 0.1      | 0.01      | 0.20     | 0.095    |
| 1        | 0.01      | 0.20     | 0.095    |
