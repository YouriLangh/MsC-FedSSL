What if we do X rounds at the start of only fully labeled training to learn meaningful representations first? I think i've seen this done somewhere. *==>* *Answered*
	Note: I think [[Federated Learning Model Using CBDPL for Medical Image Segmentation]] does this

Is this redistribution of models knowledge transfer?

Todo: Make it train X times first on labele dbefore unlabeled ==> Standard

We don't do iterative pseudolabeling though, we don't keep them if i'm not wrong?

We have to talk about graduation and how it will work and ask questions about the thesis prep class...

*FedMatch also operates on the idea of grouping similar clientpredictions teogether.*

I think we need random augmentations as it won't overfit our specific dataset. Check if we actually average and feedback the models after communication rounds. ==> We don't use the new weights even.

Do i include the pre-train rounds as communication rounds? E.g: if 5 comm rounds and 2 pretrain do i do 3 total rounds after pretrain or 5 still?

Todo:
- ~~Read the papers~~, ~~reintroduce self to topic~~, ~~add to the code that it runs X times first~~. *Re-check our options* n ~~see if new ideas appear/have appeared~~
Pretraining rounds might not be needed, though its a nice option to enforce. It automatically kinda does this.