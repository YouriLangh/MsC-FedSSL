

> Note: I think [[Federated Learning Model Using CBDPL for Medical Image Segmentation]] does this


Is this redistribution of models knowledge transfer?


We don't do iterative pseudolabeling though, we don't keep them if i'm not wrong?

We have to talk about graduation and how it will work and ask questions about the thesis prep class...

*FedMatch also operates on the idea of grouping similar clientpredictions teogether.*

I think we need random augmentations as it won't overfit our specific dataset. Check if we actually average and feedback the models after communication rounds. ==> We don't use the new weights even.

Do i include the pre-train rounds as communication rounds? E.g: if 5 comm rounds and 2 pretrain do i do 3 total rounds after pretrain or 5 still?

Todo:
- ~~Read the papers~~, ~~reintroduce self to topic~~, ~~add to the code that it runs X times first~~. *Re-check our options* n ~~see if new ideas appear/have appeared~~
Pretraining rounds might not be needed, though its a nice option to enforce. It automatically kinda does this.

The idea of a singular global model does not work. Innovative idea generation comes from plenty of ideas ==> More data = more ideas.
The idea is to combine as much data as we can to allow for a public global model while not affecting the performance of the local models.

Are we targetting specialized datasets or do we just want to offer the best model possible to the clients who require a general model.


Adaptive threshold (DDRFED, CBAFed) + Select only K best  FROM largest cluster (Clustering clients before averaging: [[Benchmarking-Semi-supervised-Federated-Learning]], [[FedECG]], [[(RSCFed) Random Sampling Consensus Federated Semi-supervised Learning]], [[Overcoming Client Data Deficiency in Federated Learning by Exploiting Unlabeled Data on the Server]], [[(SemiFed) Semi-supervised Federated Learning with Consistency and.pdf]]) (==> Adaptive clustering?) + And do we then average these weights or do something else? + Server will add noise & more info which will be filtered out by students during training

Idea behind it is: when young you get alot of bad info( low threshold) but it gets filtered out over the years. And, if we want to make a general model, we have to combine the ideas of the masses. *Perhaps use PCA to ease clustering?*

What about the idea of quizzing? Train models for X epochs, and then combine K top models which answer X questions correctly

People have tried different ways of averaging the weights such as EMA and all.