> Update the threshold over the epochs or communication rounds? ==> Good experiment
> Adaptive threshold on server too?
> Should i subdivide the data first such that server has the same dataset or not?
> How to display loss curves of models that sometimes did not train?
> - Perhaps have to use an epoch field for the training_logs as sometimes models might not train.
> Semi-supervised expects the model to have some labeled data, so do we add some labeled data on the server side or do i call it self-supervised? --> Mishmash of stuff.
> Adaptive lambda_u?

We have model collapse. Our model/Implementation is not adding any valuable information and guesses, similarly to the standard federated model. How do we fix this...
- Tunable params: Lambda_u / Threshold, Nr round/epochs **Pretrain rounds**
- We accept too many pseudolabels too early on. Either pretrain OR increase the threshold and reduce initial lambda_u.
- Investigate how data is split up across clients. Perhaps check accuracy of each client on the test set? ==> done
-  Improper label handling (e.g., labels shifted, or all zeros)?
- **Inspect pseudolabel confidence** (FixMatch):
    - Are most pseudolabels for class 7?
### Recommended Fixes:

- **Add class distribution monitoring** during training to catch prediction collapse early.
    
- For FixMatch:
    
    - Make sure the **confidence threshold** isn't too strict (e.g., try 0.8 instead of 0.95).
        
    - Ensure strong and weak augmentations are configured correctly.
        
    - Try warm-starting FixMatch with a short supervised pretraining phase.
### ðŸ“Š What to check or try:

1. **Plot the prediction distribution** (softmax output histogram) â€“ youâ€™ll probably see a spike at one class.
    
        
3. **Inspect pseudolabel confidence** (FixMatch):
    
    - Are most pseudolabels for class 7?
        
4. **Add entropy regularization** or **label smoothing**.
    
5. **Try FedAvg with IID data** as a sanity check.

#### 2. **Misconfiguration or training issue**

- Possible reasons include:
    
    - Improper label handling (e.g., labels shifted, or all zeros).
        
    - Incorrect loss function or optimizer setup.
        
    - Bug in client aggregation or update.
        
    - FixMatch pseudolabels never passing confidence threshold, causing the model to overfit to whatever small signal it does get.
        
    - Improper weighting of supervised vs. unsupervised loss in FixMatch (`lambda_u` might be too high or low).