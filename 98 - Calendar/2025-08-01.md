Discussion Points:
- Talk about the slides & discuss if anything need be changed
~~- Talk about the fact that i kept backbone & head & added averaging ==> significantly dropped our result scores~~
~~- I do not want to use HPC, I'd rather use Isabelle (mention the fact committee comes together 3x a year, might be faster this way)~~
- ~~Added ordering of STM ask about the popping of the same sample n such.~~
- ~~Added non-iid~~
- ~~Added Optuna~~
- ~~No pytorch lightning, too much refactor I think.~~
- ~~Discuss results achieved in [[#Results]]~~
- ~~Discuss fact that I want to make a list of tests I think are needed and then just run these & start writing meanwhile.~~
- Ask about validation set & k-fold validation for Optuna?

*Questions:*
if i change stm to epochs, do i need to reset it every round?
- Maybe yes maybe no
if i chqnge threshold to alter per epoc do i need to reset it every round?
- Final round should be final round
Ask about research questions.
We ar ejuyst doing fedAvg, but perhaps weighted or other approaches might be nice
We are giving each person same amount of data i think? 


**TODO:**
Finish slides
Fix bug in code
if i chqnge threshold to alter per epoc do i need to reset it every round?
- Final round should be final round
Move some global hyperparams to cli
Validation with optuna & k-fold
Think about tests & run some to have results 
Recap & start writing
Discussion

If using PTBXL dataset, then mention in thesis that we have to be careful of using validation set
- Add in discussion in thesis: In the STM: when the same instance gets multiple pseudolabels in the STM i replace it if the pseudolabels confidence is higher, should i do this for the non_ordered_dict or just throw it out? Because with explo/explor its possible to get same instance if u hit the odds. And argue about the different optiosn & why we chose this way. And mention that other one is also justified.
- Also mention the adapative threshold choice

#### Different tests:
Just enable or disable non-iid and make 2 linecurves for both situations containning all models.

And a table with diff alpha values and i report mean & variances of the different runs (where each run has different alpha)
## Results
### BEFORE AVERAGING HEAD WEIGHTS
**WITH STM & keep prob = 1**
#### No ordered dictionary
Test Accuracy: 0.6576
Confusion Matrix:
[[ 830    0   10    1    0    4    7    0    2    0]
 [  10 1126   37   23  534   14   59   87   36  122]
 [   0    4  742    3    2    0    3    2    2    0]
 [   0    1    4  748    0   91    1    0   11    7]
 [   0    0    0    0  323    0    0    0    0    0]
 [   0    0    1    5    0  519   10    0    9    0]
 [  18    2   34    8   43   13  862    0   10    1]
 [ 121    2  193  214   77  244   15  939  424  871]
 [   1    0   11    8    0    7    1    0  480    1]
 [   0    0    0    0    3    0    0    0    0    7]]
F1 Macro: 0.6459, F1 Weighted: 0.6443, Recall Macro: 0.6522
STM buffer length (client 0): 200

#### Ordered dictionary
> Note: I think this is after averaging heads, bc results === 

Test Accuracy: 0.6732
Confusion Matrix:
[[ 860    0   10    0    0    8   12    0    9    4]
 [  36 1130   89   80  642   67  110  266  155  642]
 [  12    3  859   20    2   13   20    7   17    7]
 [   0    1   14  873    0  142    0    1   34   12]
 [   0    0    0    0  315    0    3    0    1    2]
 [   0    0    0    3    0  547    3    0    5    1]
 [  21    1    2    0    9    3  804    0    2    1]
 [  45    0   47   25   14  101    2  754  195  297]
 [   6    0   11    9    0   11    4    0  556    9]
 [   0    0    0    0    0    0    0    0    0   34]]
F1 Macro: 0.6632, F1 Weighted: 0.6592, Recall Macro: 0.6681
STM buffer length (client 0): 200

### AFTER AVERAGING HEAD WEIGHTS
**WITH STM & keep prob = 1**

#### No ordered dictionary

Test Accuracy: 0.6903
Confusion Matrix:
[[ 639    0    5    0    0    0    6    0    0    0]
 [  28 1130  122   48  523   31  115  176   62  231]
 [   0    2  743    3    2    2    6    4    2    0]
 [   6    3   29  868    0   66    2    0   54   19]
 [   0    0    1    0  401    0    5    0    0    0]
 [   3    0    2   45    0  720   29    0   42    4]
 [   6    0    0    0    8    1  781    0    3    1]
 [ 292    0  108   36   43   65    4  847  169  619]
 [   6    0   22   10    0    7   10    1  642    3]
 [   0    0    0    0    5    0    0    0    0  132]]
F1 Macro: 0.6902, F1 Weighted: 0.6866, Recall Macro: 0.6872
#### Ordered dictionary
Test Accuracy: 0.6732
Confusion Matrix:
[[ 860    0   10    0    0    8   12    0    9    4]
 [  36 1130   89   80  642   67  110  266  155  642]
 [  12    3  859   20    2   13   20    7   17    7]
 [   0    1   14  873    0  142    0    1   34   12]
 [   0    0    0    0  315    0    3    0    1    2]
 [   0    0    0    3    0  547    3    0    5    1]
 [  21    1    2    0    9    3  804    0    2    1]
 [  45    0   47   25   14  101    2  754  195  297]
 [   6    0   11    9    0   11    4    0  556    9]
 [   0    0    0    0    0    0    0    0    0   34]]
F1 Macro: 0.6632, F1 Weighted: 0.6592, Recall Macro: 0.6681