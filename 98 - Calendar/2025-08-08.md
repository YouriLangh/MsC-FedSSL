**TODO**
Validation with optuna & k-fold (this means that data splitting has to be done before already kinda, since trainloader cant just take from the folder) This means that train_data in fedssl or supervised baseline has to be from a function which i alter.
~~Move todos~~
~~**Fix Model collapse again...**
- <span style="color:rgb(255, 0, 0)">It is possible that the data redistribution & stuff with the STM is wrong ==> leading to higher supervisedloss? THIS IS MOST LIKELY THE ISSUE, IT IS SOMETHING ABOUT PUTTING PSEUDOLABELS IN LABELED LOADER, PERHAPS USE WEIGHTED LOSS WITH CONFIDENCES, BUT REMEMBER THAT REMOVING STM REDUCES ACCURACY EVEN MORE</span>
- **Perform same tests but for cifar10 n then mail to Simon.**
	**"I fixed a bug with data splitting but nevertheless, performance is 1/2 asgood as just supervised & higher supervisedloss? than unsupervised"**
~~Think about tests & run some to have results ~~
Recap & start writing
Test sample project on hpc from Simon 
- Include more & more libraries n then switch to my code
~~Add ability to store different data for all teh tests:~~
~~Use 1 file per run id and then a helper.~~
 ```python
all_results = []
for alpha in [0.1, 0.5, 1.0]:
    for labeled_fraction in [0.01, 0.05, 0.1]:
        for lambda_u in [0.0, 0.1, 0.7, 1.0]:
            result = run_experiment(alpha, labeled_fraction, lambda_u)
            all_results.append(result)

```
~~*==> Alter the visualization code & result saving code.*~~ ~~**NOT FULLY DONE YET**~~
~~Alter results saving code to perhaps have some identifying fields for them as can be found in [[Store tests]]. Like 'lambda_u : 0.1' etc~~
~~Add code to run all the tests~~
~~**TODO: clean up teh pathjs for saving plots n shit**~~
**Revisit all code n make sure n obugs**
~~**Add normalizations for each dataset:**
**Cifar10: T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])**~~

Found: Lambda_u cant be taken too high to not overfload the feature extractor, pretraining has t obe done sufficiently long to extract good features, can't just add pseudolabel of the same class to buffer --> predict only 1 class
Perhaps best to add max labels per class.