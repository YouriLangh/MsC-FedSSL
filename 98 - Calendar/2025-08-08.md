**TODO**
Validation with optuna & k-fold (this means that data splitting has to be done before already kinda, since trainloader cant just take from the folder) This means that train_data in fedssl or supervised baseline has to be from a function which i alter.
**Fix Model collapse again...**
- <span style="color:rgb(255, 0, 0)">It is possible that the data redistribution & stuff with the STM is wrong ==> leading to higher supervisedloss? THIS IS MOST LIKELY THE ISSUE, IT IS SOMETHING ABOUT PUTTING PSEUDOLABELS IN LABELED LOADER, PERHAPS USE WEIGHTED LOSS WITH CONFIDENCES, BUT REMEMBER THAT REMOVING STM REDUCES ACCURACY EVEN MORE</span>
- Are samples even ever added back to unlabeled loader?
Rather than adding to labeled_dataset
	- Keep `true_labeled_dataset` immutable.
	    
	- Create `pseudo_labeled_dataset` (from STM contents).
	    
	- Build `working_labeled_loader` by concatenating `true_labeled_dataset` + `pseudo_labeled_dataset` **but track sample origins** (flag `is_pseudo`) and use per-sample weighting in the loss (downweight pseudo samples: e.g., 0.1â€“0.5).
	    
	- **Do not remove** those examples from the unlabeled dataset until they have been in STM for `k` rounds with _high_ stability/confidence.
- Perform same tests but for cifar10 n then mail to Simon.
	"I fixed a bug with data splitting but nevertheless, performance is 1/2 asgood as just supervised & higher supervisedloss? than unsupervised"
~~Think about tests & run some to have results ~~
Recap & start writing
Test sample project on hpc from Simon 
- Include more & more libraries n then switch to my code
~~Add ability to store different data for all teh tests:~~
~~Use 1 file per run id and then a helper.~~
 ```python
all_results = []
for alpha in [0.1, 0.5, 1.0]:
    for labeled_fraction in [0.01, 0.05, 0.1]:
        for lambda_u in [0.0, 0.1, 0.7, 1.0]:
            result = run_experiment(alpha, labeled_fraction, lambda_u)
            all_results.append(result)

```
~~*==> Alter the visualization code & result saving code.*~~ **NOT FULLY DONE YET**
~~Alter results saving code to perhaps have some identifying fields for them as can be found in [[Store tests]]. Like 'lambda_u : 0.1' etc~~
~~Add code to run all the tests~~
Revisit all code n make sure n obugs
Add normalizations for each dataset:
Cifar10: T.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])
