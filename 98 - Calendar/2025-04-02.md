[cifar10-resnet/main.py at master · matthias-wright/cifar10-resnet · GitHub](https://github.com/matthias-wright/cifar10-resnet/blob/master/main.py)

https://github.com/minglllli/CBAFed


In some papers they finetune global model for X epochs, and also have Y communication rounds, is this needed?










To create an evaluation and implementation pipeline for FedSSL based on the CBAFed paper, you can follow these steps:

**1. Understanding the Federated Semi-Supervised Learning (FSSL) Problem Setting:**

- First, acknowledge that the paper focuses on a specific FSSL scenario: **few clients have fully labeled data (labeled clients), and the majority of clients have only unlabeled data (unlabeled clients)**. This is the "third line" of FSSL methods discussed in the paper.
- Understand the key challenges in this setting, as outlined in the paper:
    - **Lack of labeled data in unlabeled clients:** This makes training inherently difficult without label guidance.
    - **Non-Independent and Identically Distributed (Non-IID) data:** Labeled and unlabeled clients can have divergent class distributions, leading to inaccurate supervisory signals if models trained on labeled data are directly applied to unlabeled data.
    - **Catastrophic forgetting:** Models trained on unlabeled clients using pseudo-labels might forget the knowledge learned from labeled clients, leading to a drop in accuracy.

**2. Data Preparation and Simulation of Federated Environment:**

- **Choose Benchmark Datasets:** The paper evaluates CBAFed on several datasets: **SVHN, CIFAR-10, CIFAR-100, Fashion MNIST, and ISIC 2018 Skin**. Select one or more of these datasets for your evaluation.
- **Simulate Data Partitioning:** Replicate the Non-IID data partitioning used in the paper. They use a **Dirichlet distribution (Dir(γ) with γ = 0.8)** to generate heterogeneous data splits across clients. You'll need to implement a function that distributes the data among a fixed number of clients (e.g., 10 clients: one labeled and nine unlabeled, as in the paper) according to this distribution.
- **Designate Labeled and Unlabeled Clients:** Following the paper's setup, designate a small number of clients (e.g., one) as having the full labeled dataset and the remaining clients as having only unlabeled data.
- **Pre-processing:** Implement any necessary data pre-processing steps as described in the supplementary material of the paper (this might include normalization, resizing, etc.).

**3. Implementing Baseline FSSL Methods:**

- To evaluate the effectiveness of CBAFed, you need to implement the state-of-the-art FSSL methods that the paper compares against: **RSCFed, FedIRM, and Fed-Consist**.
- Pay close attention to the specific implementation details mentioned in the paper for these baselines, especially how they handle the Non-IID setting. For instance, the paper mentions adjusting the weight of the labeled client for FedIRM and Fed-Consist. RSCFed utilizes sub-consensus models.
- Also implement **standard Federated Averaging (FedAvg) trained with all clients (as an upper bound) and with only the labeled client (as a lower bound)** for reference. Additionally, consider implementing **FedAvg with the proposed residual weight connection on the labeled client (FedAvg+)** to assess the impact of this component independently.
- Implement **Fed-Consist with the proposed fixed pseudo labeling (Fed-Consist+)** to isolate the effect of the pseudo labeling strategy.

**4. Implementing the CBAFed Method:**

- Implement the core components of CBAFed:
    - **Residual Weight Connection:** Implement the skip connection mechanism for both local training on labeled clients and global model aggregation. This involves maintaining a model from a previous epoch/communication round and blending it with the current model based on the scaling parameter α (α1 for local, α2 for global) and skip interval 's'.
    - **Warm-up Stage:** Train the global model using only the labeled clients for the initial 'P' communication rounds using FedAVG with residual weight connection.
    - **Fixed Pseudo Labeling:** For unlabeled clients, after receiving the global model, generate pseudo-labels for their entire unlabeled dataset based on the model's predictions. Select a subset of these pseudo-labeled data points based on a confidence threshold (initially, this threshold will be influenced by the class balanced adaptive threshold calculated in the previous round, or a starting threshold). This fixed set `D̃µ` will be used for training in the current communication round.
    - **Class Balanced Adaptive Threshold (CBAPL):**
        - In each communication round, after local training, each client (labeled and unlabeled) reports the number of training data for each class (`σℓt(c)` for labeled, `σµt(c)` for unlabeled) back to the central server. For unlabeled clients, this count should reflect the data used for training in that round (including discovered tail class data).
        - The central server calculates the total number of training data for each class `σt(c)` by summing the counts from all clients.
        - Compute the empirical distribution of training data `p̃t(c)`.
        - Calculate the standard deviation of this empirical distribution `std(p̃t)`.
        - Determine the class-specific threshold `τt,c` using the formula: `τt,c = p̃t(c) + τ − std(p̃t)`, where `τ` is the pre-defined threshold base. Remember to use the modified `p̃t(c)` as `p̃t(c) = (σt(c) / ∑i σt(i)) * (C / 10)`.
        - Apply an upper bound `τh` (e.g., 0.95) to the threshold: `Tt+1(c) = min(τt,c, τh)`.
        - The central server then sends these class-specific adaptive thresholds to all unlabeled clients.
        - Unlabeled clients then update their fixed pseudo-labeled training set `D̃t+1,µ` based on these new thresholds.
    - **Discovery of Unlabeled Data from Tail Classes:**
        - For unlabeled data points whose maximum confidence is below the class-specific threshold, identify the class with the second highest confidence (`ŷµ′i`).
        - If the empirical distribution of this second highest confidence class `p̃t(ŷµ′i)` is below a certain threshold (β/C, where β is a hyperparameter), include this data point in the training set with the second highest confidence label.
        - The final training dataset for unlabeled clients `Dtrain µ` is the union of the fixed pseudo-labeled set `D̃µ` and the discovered tail class data `Dtail µ`.
    - **Local Training:** Labeled clients train on their labeled data using a standard supervised loss (e.g., cross-entropy) with the residual weight connection. Unlabeled clients train on their `Dtrain µ` using a cross-entropy loss between the predicted probabilities and the pseudo-labels.
    - **Global Model Aggregation:** The central server aggregates the local models received from all clients using weighted averaging. The weights `wit` are proportional to the number of training samples used by each client in the current round (`|Dℓ|` for labeled, `|Dtrain t,i|` for unlabeled). Apply the residual weight connection during aggregation as well.

**5. Setting up the Evaluation Pipeline:**

- **Define Evaluation Metrics:** Use the **test accuracy** as the primary evaluation metric, as done in the paper.
- **Configure Training Parameters:** Set the hyperparameters according to the paper's "Implementation Details". This includes the optimizer (SGD with momentum 0.9), learning rate (refer to supplementary), local training epochs (e.g., 11 for labeled, 1 for unlabeled), and the total number of communication rounds (e.g., 200 as in the pseudo labeling strategy study). Note the values for α1, α2, s, the initial threshold base τ, and the upper bound threshold τh (usually 0.95). Also define the hyperparameter β for tail class data discovery.
- **Run Experiments:** Execute the federated learning process for CBAFed and all the baseline methods. Record the test accuracy of the global model after each communication round.
- **Monitor Training:** Track the training loss and accuracy on both labeled and unlabeled clients if needed for debugging and analysis.

**6. Performing Evaluation and Analysis:**

- **Compare Results:** Compare the final test accuracy of CBAFed with the baseline methods on the chosen datasets. Analyze the percentage improvements achieved by CBAFed.
- **Ablation Studies:** To understand the contribution of each component of CBAFed, perform ablation experiments as shown in Table 4. This involves running CBAFed with one or more components removed (e.g., without residual weight connection, without class balanced pseudo labeling, without tail class data discovery) and comparing the performance.
- **Hyperparameter Tuning:** Conduct experiments to study the sensitivity of CBAFed to key hyperparameters like the threshold base τ, the upper bound threshold τh, and the tail class selection parameter β, as illustrated in Figure 4.
- **Analyze Pseudo Label Accuracy (Optional but Recommended):** You can also track the accuracy of the generated pseudo-labels on a held-out set of unlabeled data (if available or if you can simulate a validation set within the unlabeled data) over the communication rounds to understand how the quality of pseudo-labels evolves.
- **Visualize Results:** Plot the test accuracy curves over communication rounds for CBAFed and the baselines to visualize their learning progress and stability.

**7. Documenting and Reporting:**

- Document all the steps of your implementation and evaluation pipeline, including the specific configurations, hyperparameter settings, and the results obtained.
- Clearly report the performance of CBAFed and the baseline methods, highlighting the improvements achieved by CBAFed.
- Discuss the findings from your ablation studies and hyperparameter analysis to provide insights into the effectiveness and robustness of the proposed method.

By following these steps, you can effectively create an evaluation and implementation pipeline for FedSSL based on the CBAFed paper and gain a thorough understanding of its strengths and limitations. Remember to refer back to the paper's methodology and experimental setup for specific details and guidance during your implementation.