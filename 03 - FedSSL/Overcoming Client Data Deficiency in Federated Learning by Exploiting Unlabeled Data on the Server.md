#FedSSL 
Self not Semi
Uses entropy of each client to see how unreliable they are
Many clients, server has more unlabelled data (public data or smth)

Doesnt address non-iid? Nvm they use Dirichlet distributions on each client.
If clients have lots of data --> use average of their params for the new global model params
Rather than just using hte old model and updating this (the client models are used to make predictions n give pseudolabels, so they're seen as the teachers here) --> Clients models have to all be sent to server every round. OOf
![[Exploiting_Unlabeled_Data_on_Server.png]]



1. **The server starts with the server model from the previous round**.
    
2. **The server has a collection of models sent by various clients** after they have trained the previous server model on their local labeled datasets.
    
3. **The server also has its own unlabeled dataset**.
    
4. The key idea of FedDS is to use this **unlabeled server data to improve the server model** by leveraging the knowledge from the different client models. This is done through **entropy-weighted ensemble distillation (EED)**.
    
    - For each piece of unlabeled data on the server, the server uses all the received client models to make predictions.
    - Each client model's prediction is then **weighted based on its uncertainty**, measured by entropy. **Predictions with lower entropy (more confidence) get higher weights**, while predictions with higher entropy (less confidence) get lower weights. This helps to give more importance to potentially more accurate predictions from the clients.
    - These weighted predictions from all clients are then combined to create a **pseudo-label** for that unlabeled server data point.
5. Simultaneously, FedDS employs **self-supervised learning (SSL)** on the same unlabeled server data.
    
    - **SSL is a way for the model to learn useful features from the data itself, without needing explicit labels**. In the experiments, the paper uses **image rotation prediction** as the SSL task, where the model learns to predict how much an image has been rotated.
6. The server model is then **trained using both the pseudo-labels generated through EED and the self-supervised learning task**.
    
    - A **cross-entropy loss** is calculated between the server model's predictions and the pseudo-labels.
    - A separate **self-supervised loss** is calculated based on the SSL task (e.g., how well the model predicts the rotation of an image).
    - These two losses are combined (with a weighting factor) and used to **update the parameters of the server model** through optimization.
7. After this update phase, the **improved server model is then sent back to the clients** for the next round of local training.
    

By performing EED and SSL simultaneously on the server's unlabeled data, FedDS aims to create a more robust and generalized server model, especially in scenarios where clients have limited or unreliable data. The EED process leverages the collective knowledge of the clients while giving more weight to more confident predictions, and the SSL process helps the server model learn general features from the unlabeled data.

Overcoming_Client_Data_Deficiency_in_Federated_Learning_by_Exploiting_Unlabeled_Data_on_the_Server