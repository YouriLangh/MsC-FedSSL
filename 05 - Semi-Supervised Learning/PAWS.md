#SEMI-SL 

Kinda SSL kinda Self-SL
The augmented images are not fed through a model (parametrized), rather the most similar samples in the support set are used to assign probabilities for each class. This then gives a soft-pseudo label, which is augmented with a temperature (makes the distribution more sharp). The probabilities are a weighted average of the true labels of the samples in the supporting mini-batch. Weighted by how similar the augmented views are to the ones in the sample set. This results in a probability distribution over all possible classes, hence the term "soft" pseudolabel.

The PAWS (Predicting View Assignments with Support Samples) methodology creates **soft pseudolabels** for unlabeled images by using a **small set of randomly sampled labeled images**, which form a **support minibatch**. Here's a breakdown in simpler terms:

**Pseudolabel Creation:**

Imagine you have an unlabeled image, and you create two different slightly altered versions of it (like taking two different crops or applying different color changes). The goal is to teach the model that these two versions belong to the same underlying object, even though they look slightly different. To do this, PAWS assigns "soft guesses" (pseudolabels) to these two versions and then encourages the model to make these guesses as similar as possible.

Here's how these soft pseudolabels are generated:

- **Representations:** First, the model looks at each of the two versions of the unlabeled image and converts them into numerical representations (like a fingerprint). It does the same for the images in the **support minibatch** of labeled data.
- **Similarity Comparison:** Next, for each representation of the unlabeled image versions, the method compares it to the representations of all the labeled images in the **support minibatch**. It uses a **similarity metric** (specifically, a temperature-scaled cosine similarity) to determine how "close" the representation of the unlabeled view is to each labeled representation. Think of it like finding the most similar "fingerprints" in the labeled set for each unlabeled view's "fingerprint."
- **Weighted Average of Labels:** The **soft pseudolabel** for an unlabeled image view is then created by taking a **weighted average of the true labels** of the images in the **support minibatch**. The weights are determined by the similarity scores calculated in the previous step. If an unlabeled view's representation is very similar to the representation of a labeled image of a "cat," then the "cat" label will have a higher weight in the soft pseudolabel for that unlabeled view. This results in a probability distribution over all possible classes, hence the term "soft" pseudolabel.

**Role of the Minibatch:**

The **support minibatch** of labeled images plays a crucial role in this process:

- **Providing Semantic Information:** The labeled images in the support minibatch are the source of the class information that is used to create the pseudolabels for the unlabeled data. Without these labeled samples, the model wouldn't know anything about the actual classes of objects.
- **Non-Parametric Pseudo-labeling:** The pseudolabels are generated **non-parametrically** because they directly depend on the representations of the labeled samples in the current support minibatch rather than being predicted by a separate, learned classifier. This means the pseudolabels can change depending on which labeled samples are included in the support minibatch at each training step.
- **Regularization through Random Sampling:** By randomly sampling a different support minibatch at each iteration, PAWS avoids overfitting to a specific set of labeled examples and encourages the model to learn more general representations.

**Overall Methodology:**

The PAWS methodology works as follows:

1. **Input:** It takes a large dataset of unlabeled images and a smaller **support dataset** of labeled images.
2. **View Generation:** For each unlabeled image, it creates multiple views through data augmentations. Typically, it uses a **multi-crop strategy** with two large crops and several smaller crops.
3. **Support Sampling:** At each training iteration, a **support minibatch** of labeled images is randomly sampled.
4. **Representation Learning:** The model's encoder network processes the different views of the unlabeled image and the labeled images in the support minibatch to create their respective representations.
5. **Pseudolabel Assignment:** For each view of the unlabeled image, a **soft pseudolabel** is generated by comparing its representation to the representations of the labeled images in the **support minibatch** and taking a weighted average of their true labels based on similarity.
6. **Consistency Loss:** The model is trained to minimize a **consistency loss**, which measures how different the soft pseudolabels assigned to different views of the same unlabeled image are. Specifically, it minimizes the cross-entropy between the prediction of one view and the **sharpened** prediction of another view. **Sharpening** makes the soft pseudolabels more confident (closer to a one-hot distribution).
7. **Mean Entropy Maximization (ME-MAX):** An additional regularization term, ME-MAX, is used to encourage the average of the sharpened predictions across all unlabeled representations to be close to a uniform distribution, preventing the model from focusing on only a few classes.
8. **Training Objective:** The overall training objective combines the consistency loss and the ME-MAX regularization term.
9. **Fine-tuning (Optional):** After pre-training, the learned representations can be optionally fine-tuned using only the labeled data for a specific downstream task like image classification. PAWS also demonstrates strong performance using the pre-trained representations directly in a nearest-neighbor classifier (PAWS-NN) without fine-tuning.

In essence, PAWS leverages a small amount of labeled data to guide the learning of useful visual features from a large amount of unlabeled data by ensuring that different views of the same image are assigned similar pseudolabels based on their similarity to the labeled support samples.


In the PAWS approach, the **anchor sample** and the **positive sample** are different **views of the same unlabeled image**, created using random data augmentations. They play a central role in the method's self-supervised learning objective within the semi-supervised setting. Here's a breakdown of their roles:

- **Anchor View (x̂i):** This is one version of the unlabeled image generated through a set of data augmentations. It acts as the central reference point for comparison.
- **Positive View (x̂+i):** This is another distinct version of the _same_ unlabeled image, generated using a _different_ (but potentially overlapping) set of data augmentations. It represents a semantically similar instance to the anchor view, as they both originate from the same underlying image.

Their primary roles in the PAWS methodology are as follows:

- **Pseudolabel Generation:** Both the anchor view and the positive view are assigned **soft pseudolabels** non-parametrically. As we discussed, these pseudolabels are generated by comparing the representations of these views to the representations of a **support minibatch** of labeled images and creating a weighted average of the support samples' true labels based on similarity.
    
- **Consistency Learning:** The core of the PAWS training objective is to ensure **consistency** between the pseudolabels assigned to different views of the same unlabeled instance. Specifically, the model is trained to minimize the cross-entropy between the prediction of the anchor view (pi) and the **sharpened** prediction of the positive view (ρ(p+i)), and vice versa: H(ρ(p+i), pi) + H(ρ(pi), p+i).
    
    - The **prediction of one view serves as the target for the other view**, after the target prediction has been **sharpened**. This encourages the model to learn representations that are invariant to the data augmentations applied to create the different views, as they should ideally lead to similar (and confidently predicted) pseudolabels.
    - By comparing the predictions of the anchor and positive views of the _same_ unlabeled image, PAWS learns to group together different augmented versions of the same underlying content in the representation space.
- **Preventing Representation Collapse:** Comparing different views of the same image and enforcing consistency between their (sharpened) pseudolabels helps to avoid the trivial solution where the model collapses all representations to a single, meaningless vector. The theoretical guarantee provided by Proposition 1 states that under class-balanced sampling of the support set and target sharpening, representational collapse leads to high-entropy predictions, while the sharpened targets are low-entropy, ensuring a non-zero gradient.
    

In the **multi-crop strategy**, this concept extends beyond just two views:

- There are two large crop views and several small crop views of each unlabeled image.
- Each **small crop** has the **two large crops** as its positive views. The target for a small crop's prediction is the average of the sharpened predictions of the two large crops.
- Each **large crop** has the **other large crop** as its positive view.

Therefore, the anchor and positive samples (or in the multi-crop case, the multiple views and their designated positive view(s)) are fundamental to how PAWS learns from unlabeled data by enforcing consistency of predicted soft pseudolabels across different augmented versions of the same image, guided by the semantic information provided by the labeled support samples.



### Example:

Yes, let's illustrate how a soft pseudolabel is generated in PAWS when you have 3 samples of each class in your support minibatch. For simplicity, let's assume we have a classification problem with **two classes**: Class 0 and Class 1.

Suppose the support minibatch $z_S$ contains: 
-  3 representation vectors from images belonging to Class 0, with one-hot labels $y$. Let's denote these representations as $z_{s0}, z_{s1}, z_{s2}$. 
-  3 representation vectors from images belonging to Class 1, with one-hot labels $y$. Let's denote these representations as $z_{s3}, z_{s4}, z_{s5}$. 

Now, consider an **unlabeled anchor view** whose representation is $z_i$. The soft pseudolabel $p_i$ for this anchor view is generated using the similarity classifier $\pi_d(z_i, z_S)$, which is given by:

$p_i := \pi_d(z_i, z_S) = \sigma_\tau (z_i z_S^T) y_S = \sum_{(z_{sj} ,y_j) \in z_S} \left( \frac{d(z_i, z_{sj})}{\sum_{z_{sk} \in z_S} d(z_i, z_{sk})} \right) y_j$

where $d(a, b) = \exp(a^T b / |a||b|\tau)$ is the temperature-scaled cosine similarity, $\tau > 0$ is the temperature, and $y_j$ is the one-hot label of the support sample $z_{sj}$.

Let's make some hypothetical similarity scores between the unlabeled view $z_i$ and each of the support samples:

- $d(z_i, z_{s0}) = 0.8$
- $d(z_i, z_{s1}) = 0.7$ 
- $d(z_i, z_{s2}) = 0.6$ 
- $d(z_i, z_{s3}) = 0.2$ 
- $d(z_i, z_{s4}) = 0.3$ 
- $d(z_i, z_{s5}) = 0.4$ 

To get the weights, we first need to sum all these similarity scores: $\sum_{z_{sk} \in z_S} d(z_i, z_{sk}) = 0.8 + 0.7 + 0.6 + 0.2 + 0.3 + 0.4 = 3.0$

Now, we calculate the weight for each support sample:

- Weight for $z_{s0}$ (Class 0): $w_0 = 0.8 / 3.0 \approx 0.267$ 
- Weight for $z_{s1}$ (Class 0): $w_1 = 0.7 / 3.0 \approx 0.233$ 
- Weight for $z_{s2}$ (Class 0): $w_2 = 0.6 / 3.0 = 0.200$
- Weight for $z_{s3}$ (Class 1): $w_3 = 0.2 / 3.0 \approx 0.067$ 
- Weight for $z_{s4}$ (Class 1): $w_4 = 0.3 / 3.0 = 0.100$
- Weight for $z_{s5}$ (Class 1): $w_5 = 0.4 / 3.0 \approx 0.133$ 

Finally, the soft pseudolabel $p_i$ is the weighted average of the one-hot labels:

$p_i = (w_0 \times) + (w_1 \times) + (w_2 \times) + (w_3 \times) + (w_4 \times) + (w_5 \times)$

``` Note
This is a vector now, where we use one-hot encoding.
[1, 0] represents a sample from class 0.
```
$p_i = ([0.267, 0] + [0.233, 0] + [0.200, 0]) + ([0, 0.067] + [0, 0.100] + [0, 0.133])$

$p_i = [0.700, 0] + [0, 0.300]$

$p_i = [0.700, 0.300]$

So, in this example, the **soft pseudolabel** for the unlabeled anchor view $z_i$ is **$[0.700, 0.300]$**. This indicates that, based on its similarity to the support samples, the model predicts the unlabeled image has a **70% probability of belonging to Class 0** and a **30% probability of belonging to Class 1**.

The temperature $\tau$ in the similarity metric plays a crucial role in how "soft" or "hard" these probabilities are. A smaller $\tau$ will make the distribution more peaked towards the most similar support samples, resulting in a more confident (sharper) pseudolabel. The paper also explicitly mentions using a temperature $\tau = 0.1$ for the cosine similarity.

![[PAWS.pdf]]